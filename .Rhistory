knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
df <- read_csv('adult_census.csv')
knitr::opts_chunk$set(message = FALSE)
library(tidyverse)
df <- read_csv('adult_census.csv')
df <- df %>%
select(-education.num, -native.country, -relationship)
df <- df %>%
rename(target = income)
df <- df %>%
mutate(target = as.factor(target))
library(caret)
set.seed(2020)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]
library(rpart)
tree_model <- rpart(target ~ ., data = df_train,
control = rpart.control(maxdepth = 3))
pred <- predict(tree_model, df_test, type = "class")
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = ">50K")
cm$overall[1]
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(tree_model)
tree_model$variable.importance
barplot(tree_model$variable.importance)
tree_model2 <- rpart(target ~ ., data = df_train,
control = rpart.control(maxdepth = 4))
pred <- predict(tree_model2, df_test, type = "class")
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = ">50K")
cm$overall[1]
library(randomForest)
install.packages("randomForest")
library(randomForest)
forest_model = randomForest(target ~ ., data=df_train, ntree = 1000)
pred <- predict(forest_model, df_test, type = "class")
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = ">50K")
cm$overall[1]
importance(forest_model)
barplot(forest_model$variable.importance)
forest_model2 = randomForest(target ~ ., data=df_train, ntree = 4)
pred <- predict(forest_model2, df_test, type = "class")
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = ">50K")
cm$overall[1]
barplot(tree_model$variable.importance)
barplot(forest_model$variable.importance)
forest_model$variable.importanc
importance(forest_model)
barplot(forest_model)
#install.packages("randomForest")
library(randomForest)
forest_model = randomForest(target ~ ., data=df_train, ntree = 1000)
importance(forest_model)
barplot(forest_model)
barplot(forest_model$importance)
forest_model$terms
forest_model2 = randomForest(target ~ ., data=df_train, ntree = 4)
pred <- predict(forest_model2, df_test, type = "class")
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = ">50K")
cm$overall[1]
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
install.packages(mlbench)
install.packages(mlbench)
install.packages(mlbench)
library(mlbench)
library(mlbench)
library(mlbench)
library(tidyverse)
data(PimaIndiansDiabetes)
df <- tibble(PimaIndiansDiabetes)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(mlbench)
install.packages("mlbench")
library(mlbench)
library(tidyverse)
data(PimaIndiansDiabetes)
df <- tibble(PimaIndiansDiabetes)
names(df)
df <- df %>%
select(-pedigree, -triceps, -pregant, -glucose)
df <- df %>%
select(-pedigree, -triceps, -pressure, -glucose)
df <- df %>%
select(-pedigree, -triceps, -pregnant, -glucose)
df <- df %>%
select(-pedigree, -triceps, -mass, -glucose)
df <- df %>%
select(-pedigree, -triceps, -pressure, -glucose)
df <- df %>%
select(-pedigree, -triceps, -pressure, -glucose)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
#install.packages("mlbench")
library(mlbench)
library(tidyverse)
data(PimaIndiansDiabetes)
df <- tibble(PimaIndiansDiabetes)
names(df)
df <- df %>%
select(-pedigree, -triceps, -pressure, -glucose)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
#install.packages("mlbench")
library(mlbench)
library(tidyverse)
data(PimaIndiansDiabetes)
df <- tibble(PimaIndiansDiabetes)
names(df)
df <- df %>%
select(-pedigree, -triceps, -pregnant, -glucose)
df <- df %>%
rename(target = diabetes)
df <- df %>%
mutate(target = as.factor(target))
library(caret)
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model1 <- train(target~., data=df_train,
method = "rpart2",
maxdepth=3)
pred <- predict(model1, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
varImp(model1)
plot(varImp(model1))
model2 <- train(target~., data=df_train,
method = "rf",
ntree = 1000)
pred <- predict(model2, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
varImp(model2)
plot(varImp(model2))
library('glmnet')
install.packages("glmnet")
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
#install.packages("mlbench")
library(mlbench)
library(tidyverse)
data(PimaIndiansDiabetes)
df <- tibble(PimaIndiansDiabetes)
names(df)
df <- df %>%
select(-pedigree, -triceps, -pregnant, -glucose)
df <- df %>%
rename(target = diabetes)
df <- df %>%
mutate(target = as.factor(target))
library(caret)
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model1 <- train(target~., data=df_train,
method = "rpart2",
maxdepth=3)
pred <- predict(model1, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
varImp(model1)
plot(varImp(model1))
model2 <- train(target~., data=df_train,
method = "rf",
ntree = 1000)
pred <- predict(model2, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
varImp(model2)
plot(varImp(model2))
install.packages("glmnet")
library('glmnet')
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model1 <- train(target~., data=df_train,
method = "glmnet")
pred <- predict(model1, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
varImp(model1)
plot(varImp(model1))
library('ada')
install.packages("ada")
library('ada')
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model1 <- train(target~., data=df_train,
method = "ada")
pred <- predict(model1, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
install.packages("bagFDA")
library('bagFDA')
install.packages("bagFDA")
library('bagFDA')
library('knn')
install.packages("knn")
library('knn')
library('caret')
df$age[is.na(df$age)] <- mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = 0.70, list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ -splitIndex,]
model1 <- train(target ~ ., data = df_train, method = "knn"
library('caret')
df$age[is.na(df$age)] <- mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = 0.70, list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ -splitIndex,]
model1 <- train(target ~ ., data = df_train, method = "knn",
preProcess = c("center", "scale"),
# Load necessary libraries
library(caret)
# Assuming `df` is your dataset and it's already loaded
# Handle missing values in the `age` column by replacing NAs with the column's mean
df$age[is.na(df$age)] <- mean(df$age, na.rm = TRUE)
# Create the data partition: 70% for training, 30% for testing
set.seed(123)  # Set seed for reproducibility
splitIndex <- createDataPartition(df$target, p = 0.70, list = FALSE)
# Split the data into training and testing sets
df_train <- df[splitIndex, ]
df_test <- df[-splitIndex, ]  # Use the complement of the split
# Train the KNN model using the training set
model1 <- train(target ~ ., data = df_train, method = "knn",
preProcess = c("center", "scale"),  # Optional: Preprocess features
tuneLength = 10)  # Tune the number of neighbors (k) using cross-validation
# Print the model summary
print(model1)
# You can now make predictions using the test set
predictions <- predict(model1, df_test)
# Evaluate model performance (e.g., accuracy for classification)
confusionMatrix(predictions, df_test$target)
library(caret)
df$age[is.na(df$age)] <- mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = 0.70, list = FALSE)
df_train <- df[splitIndex, ]
df_test <- df[-splitIndex, ]
model1 <- train(target ~ ., data = df_train, method = "knn",
preProcess = c("center", "scale"),
tuneLength = 10)  # Tune the number of neighbors (k) using cross-validation
library(caret)
df$age[is.na(df$age)] <- mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = 0.70, list = FALSE)
df_train <- df[splitIndex, ]
df_test <- df[-splitIndex, ]
model1 <- train(target ~ ., data = df_train, method = "knn",
preProcess = c("center", "scale")
library(caret)
df$age[is.na(df$age)] <- mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = 0.70, list = FALSE)
df_train <- df[splitIndex, ]
df_test <- df[-splitIndex, ]
model1 <- train(target ~ ., data = df_train, method = "knn")
pred <- predict(model1, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
library('glmnet')
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model1 <- train(target~., data=df_train,
method = "glmnet")
pred <- predict(model1, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
library('glmnet')
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model1 <- train(target~., data=df_train,
method = "glmnet")
pred <- predict(model1, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
varImp(model1)
plot(varImp(model1))
knitr::opts_chunk$set(message = FALSE)
install.packages("rattle")
library(rattle)
fancyRpartPlot(tree_model)
library(rattle)
fancyRpartPlot(tree_model)
varImp(model1)
plot(varImp(model1))
library('ada')
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model2 <- train(target~., data=df_train,
method = "ada")
pred <- predict(model1, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
library('ada')
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model2 <- train(target~., data=df_train,
method = "ada")
varImp(model)
varImp(model2)
plot(varImp(model2))
varImp(model3)
plot(varImp(model3))
library('ada')
df$age[is.na(df$age)] = mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[ splitIndex,]
model3 <- train(target~., data=df_train,
method = "ada")
pred <- predict(model3, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
varImp(model3)
library(caret)
df$age[is.na(df$age)] <- mean(df$age, na.rm = TRUE)
splitIndex <- createDataPartition(df$target, p = 0.70, list = FALSE)
df_train <- df[splitIndex, ]
df_test <- df[-splitIndex, ]
model4 <- train(target ~ ., data = df_train, method = "knn")
pred <- predict(model4, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
varImp(model4)
plot(varImp(model2))
knitr::opts_chunk$set(message = FALSE)
library(mlbench)
library(tidyverse)
library(caret)
data(PimaIndiansDiabetes)
df <- tibble(PimaIndiansDiabetes)
df <- df %>% rename(target=diabetes)
df <- df %>%
mutate(target = as.factor(target))
splitIndex <- createDataPartition(df$target, p = .70,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]
library(randomForest)
getModelInfo('ranger')$ranger$parameters
trControl = trainControl(method = "cv",
number = 7)
tuneGrid = expand.grid(mtry = 2:4,
splitrule = c('gini', 'extratrees'),
min.node.size = c(1:10))
forest_ranger <- train(target~., data=df_train,
method = "ranger",
trControl = trControl,
tuneGrid = tuneGrid)
pred <- predict(forest_ranger, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
library('lda')
install.packages('lda')
library('lda')
tuneGrid = expand.grid(maxdepth = 2:10)
trControl = trainControl(method = "cv",
number = 7)
tree_approach2 <- train(target~., data=df_train,
method = "lda",
trControl = trControl)
pred <- predict(tree_approach2, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target, positive = "pos")
cm$overall[1]
trControl = trainControl(method = "cv",
number = 7)
dda <- train(target~., data=df_train,
method = "dda",
trControl = trControl)
